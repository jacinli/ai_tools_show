from openai import  AsyncOpenAI
from dotenv import load_dotenv
import os
import json
from services.openai_tools_call import get_all_functions
from services.openai_tools_call import get_weather,MyTools,BaseTool
from langfuse import Langfuse
import uuid
from services.langfuse_service import langfuse_service

load_dotenv()


class AsyncOpenAIOut:
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.base_url = os.getenv("OPENAI_BASE_URL")
        self.oai_client = AsyncOpenAI(api_key=self.api_key, base_url=self.base_url)
        self.model = os.getenv("OPENAI_MODEL")
    

    async def gpt_stream_with_tools(self, user_message: str, model: str = None, history: list[dict] = [], system_prompt: str = ""):
        functions = get_all_functions()
        # functions = MyTools.get_tools()
        messages = []

        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.extend(history)
        messages.append({"role": "user", "content": user_message})

        # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œæ£€æŸ¥æ˜¯å¦è§¦å‘å‡½æ•°è°ƒç”¨
        response = await self.oai_client.chat.completions.create(
            model=model or self.model,
            messages=messages,
            tools=functions,
            tool_choice="auto"
        )

        choice = response.choices[0]

        # å¦‚æœæœ‰ tool è°ƒç”¨
        if choice.message.tool_calls:
            for tool_call in choice.message.tool_calls:
                name = tool_call.function.name
                args = json.loads(tool_call.function.arguments)
                print(f"å‡½æ•°è°ƒç”¨: {name} å‚æ•°: {args}")

                # æ‰§è¡Œæœ¬åœ°å‡½æ•°ï¼ˆä½ å®šä¹‰çš„ get_weatherï¼‰
                if name == "get_weather":
                    result = get_weather(**args)

                    # æ·»åŠ  assistant çš„ tool_calls å›å¤
                    messages.append({
                        "role": "assistant",
                        "tool_calls": [{
                            "id": tool_call.id,
                            "type": "function",
                            "function": {
                                "name": name,
                                "arguments": json.dumps(args)
                            }
                        }]
                    })

                    # æ·»åŠ  tool æ‰§è¡Œç»“æœ
                    messages.append({
                        "role": "tool",
                        "tool_call_id": tool_call.id,
                        "content": result
                    })

                    # é‡æ–°è°ƒç”¨ä¸€æ¬¡ OpenAIï¼Œå¾—åˆ°æœ€ç»ˆç»“æœ
                    second_response = await self.oai_client.chat.completions.create(
                        model=model or self.model,
                        messages=messages,
                        stream=True
                    )

                    async for chunk in second_response:
                        if chunk.choices[0].delta.content:
                            yield chunk.choices[0].delta.content
        else:
            # æ²¡æœ‰è§¦å‘å‡½æ•°è°ƒç”¨ï¼Œæ­£å¸¸è¾“å‡º
            if choice.message.content:
                yield choice.message.content


    async def gpt_stream_with_tools_for_base_tool(self, user_message: str, model: str = None, history: list[dict] = [], system_prompt: str = ""):

        tools = MyTools.get_tools()  # è·å–æ‰€æœ‰ tools ç»“æ„
        messages = []

        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.extend(history)
        messages.append({"role": "user", "content": user_message})

        # ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œæ£€æŸ¥æ˜¯å¦è§¦å‘å‡½æ•°è°ƒç”¨
        response = await self.oai_client.chat.completions.create(
            model=model or self.model,
            messages=messages,
            tools=tools,
            tool_choice="auto"
        )

        choice = response.choices[0]

        # å¦‚æœè§¦å‘ tool è°ƒç”¨
        if choice.message.tool_calls:
            for tool_call in choice.message.tool_calls:
                name = tool_call.function.name
                args = json.loads(tool_call.function.arguments)
                print(f"å‡½æ•°è°ƒç”¨: {name} å‚æ•°: {args}")

                # è‡ªåŠ¨è°ƒåº¦å·¥å…·
                result = await BaseTool.acall(name, args)  # å¼‚æ­¥æ”¯æŒ

                # æ·»åŠ  assistant å·¥å…·è°ƒç”¨è®°å½•
                messages.append({
                    "role": "assistant",
                    "tool_calls": [{
                        "id": tool_call.id,
                        "type": "function",
                        "function": {
                            "name": name,
                            "arguments": json.dumps(args)
                        }
                    }]
                })

                # æ·»åŠ  tool æ‰§è¡Œç»“æœ
                messages.append({
                    "role": "tool",
                    "tool_call_id": tool_call.id,
                    "content": result
                })

            # å†æ¬¡è¯·æ±‚æ¨¡å‹ä»¥è·å¾—æœ€ç»ˆå›å¤
            second_response = await self.oai_client.chat.completions.create(
                model=model or self.model,
                messages=messages,
                stream=True
            )

            async for chunk in second_response:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        else:
            if choice.message.content:
                yield choice.message.content
    
    async def gpt_stream(self, user_message: str,model: str = os.getenv("OPENAI_MODEL"),history: list[dict] = [],system_prompt: str = "") :
        messages = []
        if history:
            messages.extend(history)
        
        if system_prompt:
            messages.extend([{"role": "system", "content": system_prompt}])
        
        messages.append({"role": "user", "content": user_message})
        response = await self.oai_client.chat.completions.create(
            model=model,
            messages=messages,
            stream=True
        )
        
        async for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
                
    async def gpt_stream_with_langfuse(self, user_message: str, model: str = None, history: list[dict] = [], system_prompt: str = ""):
        trace = langfuse_service.langfuse.trace(name="gpt_stream", user_id="user-123")
        span = trace.span(name="gpt_stream_call")

        try:
            messages = []
            if history:
                messages.extend(history)

            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})

            messages.append({"role": "user", "content": user_message})

            # ğŸ” æ·»åŠ  input
            span.update(input={"messages": messages})

            response = await self.oai_client.chat.completions.create(
                model=model or self.model,
                messages=messages,
                stream=True
            )

            full_response = ""
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    full_response += chunk.choices[0].delta.content
                    yield chunk.choices[0].delta.content

            # ğŸ” æ·»åŠ è¾“å‡º
            span.end(output=full_response)
            trace.update(output=full_response)
        except Exception as e:
            span.end(output={"error": str(e)})
            trace.update(output={"error": str(e)})
            raise
    
async_openai_out = AsyncOpenAIOut()



    
if __name__ == "__main__":
    async def test_gpt_stream():
        async for chunk in async_openai_out.gpt_stream(user_message="åŒ—äº¬å¤©æ°”ï¼Ÿ",system_prompt="You are a helpful assistant."):
            # print(chunk)
            await asyncio.to_thread(print, chunk)  # åœ¨åå°çº¿ç¨‹æ‰§è¡Œ print

    import asyncio
    
    asyncio.run(test_gpt_stream())

